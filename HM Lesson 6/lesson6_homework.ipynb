{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have the latest version of sigmoid_check installed by running pip install sigmoid_check --upgrade\n",
    "from sigmoid_check.excalibur import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Extract DataFrame `df_social_media_metrics` subset for rows where 'Likes' exceed 'Shares' and store it in `highly_liked_posts`.\n",
    "@check_pandas_101\n",
    "def pandas_101(df_social_media_metrics):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    highly_liked_posts = df_social_media_metrics[df_social_media_metrics['Likes'] > df_social_media_metrics['Shares']]\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"highly_liked_posts\": highly_liked_posts}\n",
    "\n",
    "pandas_101()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use DataFrame `df_product_info` to group by 'Category', computing the minimum 'Price' for each category, storing result as `min_price_by_category`.\n",
    "@check_pandas_102\n",
    "def pandas_102(df_product_info):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    min_price_by_category = df_product_info.groupby('Category')['Price'].min()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"min_price_by_category\": min_price_by_category}\n",
    "\n",
    "pandas_102()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Apply datetime filtering on `df_online_sessions` to store sessions in the month of January 2023 in variable `january_sessions` based on 'SessionStart'.\n",
    "@check_pandas_103\n",
    "def pandas_103(df_online_sessions):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    # Ensure 'SessionStart' is in datetime format\n",
    "    df_online_sessions['SessionStart'] = pd.to_datetime(df_online_sessions['SessionStart'])\n",
    "\n",
    "    # Filter for sessions in January 2023\n",
    "    january_sessions = df_online_sessions[(df_online_sessions['SessionStart'].dt.month == 1) & \n",
    "                                          (df_online_sessions['SessionStart'].dt.year == 2023)]\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"january_sessions\": january_sessions}\n",
    "\n",
    "pandas_103()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Use DataFrame `df_employee_entries` and fill NA in 'EntryTime' with '08:00 AM', storing the result in `filled_employee_entries`.\n",
    "@check_pandas_104\n",
    "def pandas_104(df_employee_entries):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"filled_employee_entries\": filled_employee_entries}\n",
    "\n",
    "pandas_104()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Drop all columns with any NA values in DataFrame `df_transaction_records`, storing cleaned version as `non_na_transactions`.\n",
    "@check_pandas_105\n",
    "def pandas_105(df_transaction_records):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Drop all columns with any NA values\n",
    "        non_na_transactions = df_transaction_records.dropna(axis=1, how='any')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"non_na_transactions\": non_na_transactions}\n",
    "\n",
    "pandas_105()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a Series `series_ascending` from a list of numbers [1, 2, 3, 4, 5], using these values as the indices as well.\n",
    "@check_pandas_106\n",
    "def pandas_106():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Create the Series with values and indices [1, 2, 3, 4, 5]\n",
    "        series_ascending = pd.Series(data=[1, 2, 3, 4, 5], index=[1, 2, 3, 4, 5])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error creating Series: {e}\")   \n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"series_ascending\": series_ascending}\n",
    "\n",
    "pandas_106()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use the query method on DataFrame `df_financial_audit` to extract entries with 'Audit' == 'Complete' and 'Amount' > 10000, saving it as `completed_audits`.\n",
    "@check_pandas_107\n",
    "def pandas_107(df_financial_audit):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Use the query method to filter rows where 'Audit' == 'Complete' and 'Amount' > 10000\n",
    "        completed_audits = df_financial_audit.query(\"Audit == 'Complete' and Amount > 10000\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"completed_audits\": completed_audits}\n",
    "\n",
    "pandas_107()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Create a custom `percent_round` function and apply it to round the 'Progress' column in DataFrame `df_student_projects` to the nearest ten, storing result as `rounded_progress`.\n",
    "@check_pandas_108\n",
    "def pandas_108(df_student_projects):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def percent_round(x):\n",
    "        return round(x / 10) * 10\n",
    "\n",
    "    try:\n",
    "        # Apply the percent_round function to the 'Progress' column\n",
    "        rounded_progress = df_student_projects['Progress'].apply(percent_round)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error applying percent_round: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"rounded_progress\": rounded_progress}\n",
    "\n",
    "pandas_108()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Extract the elements starting from the 10th index to the 20th index from `df_user_activity`, storing the result in `sampled_user_activity`.\n",
    "@check_pandas_109\n",
    "def pandas_109(df_user_activity):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Extract rows starting from the 10th index to the 20th index (inclusive)\n",
    "        sampled_user_activity = df_user_activity.iloc[10:21]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sampled_user_activity\": sampled_user_activity}\n",
    "\n",
    "pandas_109()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use advanced indexing to set all negative values in DataFrame `df_temperature_fluctuations` to zero, storing corrected DataFrame as `non_negative_temperatures`.\n",
    "@check_pandas_110\n",
    "def pandas_110(df_temperature_fluctuations):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Set all negative values to zero using advanced (boolean) indexing\n",
    "        non_negative_temperatures = df_temperature_fluctuations.copy()\n",
    "        non_negative_temperatures[non_negative_temperatures < 0] = 0\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"non_negative_temperatures\": non_negative_temperatures}\n",
    "\n",
    "pandas_110()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# With DataFrame `df_fleet_inventory`, allocate memory efficiency by converting 'Year' column to category, saving the result as `efficient_fleet_inventory`.\n",
    "@check_pandas_111\n",
    "def pandas_111(df_fleet_inventory):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Convert 'Year' column to categorical type for memory efficiency\n",
    "        efficient_fleet_inventory = df_fleet_inventory.copy()\n",
    "        efficient_fleet_inventory['Year'] = efficient_fleet_inventory['Year'].astype('category')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"efficient_fleet_inventory\": efficient_fleet_inventory}\n",
    "\n",
    "pandas_111()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use DataFrame `df_respiratory_data` to calculate cumulative maximum of 'Pulse' within groups of 'PatientID', storing it as `grouped_cumulative_max`.\n",
    "@check_pandas_112\n",
    "def pandas_112(df_respiratory_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Group by 'PatientID' and calculate the cumulative maximum of 'Pulse'\n",
    "        grouped_cumulative_max = df_respiratory_data.groupby('PatientID')['Pulse'].cummax()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"grouped_cumulative_max\": grouped_cumulative_max}\n",
    "\n",
    "pandas_112()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Group DataFrame `df_weather_statistics` and apply aggregation to find both 'mean' and 'std' of 'Temperature', storing multi-aggregate result as `weather_stats`.\n",
    "@check_pandas_113\n",
    "def pandas_113(df_weather_statistics):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Group by the desired column (e.g., 'Location') and apply both 'mean' and 'std' to 'Temperature'\n",
    "        weather_stats = df_weather_statistics.groupby('Location')['Temperature'].agg(['mean', 'std'])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"weather_stats\": weather_stats}\n",
    "\n",
    "pandas_113()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a hierarchical index on DataFrame `df_multilayer_data` using [('Region', 'State')], saving the result as `hierarchical_multilayer`.\n",
    "@check_pandas_114\n",
    "def pandas_114(df_multilayer_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Set hierarchical index using 'Region' and 'State' columns\n",
    "        hierarchical_multilayer = df_multilayer_data.set_index(['Region', 'State'])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"hierarchical_multilayer\": hierarchical_multilayer}\n",
    "\n",
    "pandas_114()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the exponential moving average with a span of 10 on the 'Close' column in DataFrame `df_market_activity`, storing to `ema_market_activity`.\n",
    "@check_pandas_115\n",
    "def pandas_115(df_market_activity):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Calculate the exponential moving average (EMA) with a span of 10 for the 'Close' column\n",
    "        ema_market_activity = df_market_activity['Close'].ewm(span=10, adjust=False).mean()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"ema_market_activity\": ema_market_activity}\n",
    "\n",
    "pandas_115()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame `df_temperature_readings` for two sensors over the year using monthly datetime index, with lineaerly increasing temperature values from 20 to 30 for Sensor1 and 15 to 25 for Sensor2.\n",
    "@check_pandas_116\n",
    "def pandas_116():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    # Generate a monthly datetime index for one year\n",
    "    dates = pd.date_range(start='2023-01-01', end='2023-12-01', freq='MS')  # 'MS' is month start frequency\n",
    "\n",
    "    # Create linearly increasing temperature values for Sensor1 and Sensor2\n",
    "    sensor1_temps = np.linspace(20, 30, len(dates))  # From 20 to 30\n",
    "    sensor2_temps = np.linspace(15, 25, len(dates))  # From 15 to 25\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df_temperature_readings = pd.DataFrame({\n",
    "        'Sensor1': sensor1_temps,\n",
    "        'Sensor2': sensor2_temps\n",
    "    }, index=dates)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"df_temperature_readings\": df_temperature_readings}\n",
    "\n",
    "pandas_116()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use DataFrame `df_sales_tiers` to add a 'SalesTier' column assigned by binning the 'Sales' column into 'Low', 'Medium', and 'High' categories based on [0, 5000, 10000, 15000] bins, storing as `tiered_sales`.\n",
    "@check_pandas_117\n",
    "def pandas_117(df_sales_tiers):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Define the bin edges and labels for the sales tiers\n",
    "        bins = [0, 5000, 10000, 15000]\n",
    "        labels = ['Low', 'Medium', 'High']\n",
    "\n",
    "        # Create the 'SalesTier' column by binning the 'Sales' column\n",
    "        df_sales_tiers['SalesTier'] = pd.cut(df_sales_tiers['Sales'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "        # Store the result in the 'tiered_sales' variable\n",
    "        tiered_sales = df_sales_tiers\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"tiered_sales\": tiered_sales}\n",
    "\n",
    "pandas_117()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Extract month at index of DataFrame `df_time_based_events`, storing them as a new column 'EventMonth' in the DataFrame.\n",
    "@check_pandas_118\n",
    "def pandas_118(df_time_based_events):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Extract the month from the datetime index and assign it to a new column 'EventMonth'\n",
    "        df_time_based_events['EventMonth'] = df_time_based_events.index.month\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"df_time_based_events\": df_time_based_events}\n",
    "\n",
    "pandas_118()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Given a DataFrame `df_tennis_matches`, select rows using .loc where 'MatchesPlayed' > 20 and store them as `consistent_players`.\n",
    "@check_pandas_119\n",
    "def pandas_119(df_tennis_matches):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Select rows where 'MatchesPlayed' > 20\n",
    "        consistent_players = df_tennis_matches.loc[df_tennis_matches['MatchesPlayed'] > 20]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"consistent_players\": consistent_players}\n",
    "\n",
    "pandas_119()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Utilize .iloc on DataFrame `df_daily_results` to select rows by integer location, focusing on every third row, and storing result as `every_third_result`.\n",
    "@check_pandas_120\n",
    "def pandas_120(df_daily_results):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Select every third row using .iloc and store the result in every_third_result\n",
    "        every_third_result = df_daily_results.iloc[::3]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"every_third_result\": every_third_result}\n",
    "\n",
    "pandas_120()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# From DataFrame `df_survey_responses`, combine 'Question' and 'Answer' columns into a single 'Response' column, storing the result as a Series `consolidated_responses` in the format 'Question: Answer'.\n",
    "@check_pandas_121\n",
    "def pandas_121(df_survey_responses):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Combine 'Question' and 'Answer' columns into 'Response' format\n",
    "        consolidated_responses = df_survey_responses['Question'] + ': ' + df_survey_responses['Answer']\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"consolidated_responses\": consolidated_responses}\n",
    "\n",
    "pandas_121()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use DataFrame `df_server_logs` to group by 'ServerID' and calculate the sum and max of 'ResponseTime', storing result as `server_response_summary`.\n",
    "@check_pandas_122\n",
    "def pandas_122(df_server_logs):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Group by 'ServerID' and calculate the sum and max of 'ResponseTime'\n",
    "        server_response_summary = df_server_logs.groupby('ServerID')['ResponseTime'].agg(['sum', 'max'])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"server_response_summary\": server_response_summary}\n",
    "\n",
    "pandas_122()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Modify DataFrame `df_machine_operating` by adding column 'Downtime' calculated from 'EndTime' minus 'StartTime', storing as `operations_with_downtime`.\n",
    "@check_pandas_123\n",
    "def pandas_123(df_machine_operating):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Ensure 'StartTime' and 'EndTime' are in datetime format\n",
    "        df_machine_operating['StartTime'] = pd.to_datetime(df_machine_operating['StartTime'])\n",
    "        df_machine_operating['EndTime'] = pd.to_datetime(df_machine_operating['EndTime'])\n",
    "\n",
    "        # Calculate the 'Downtime' by subtracting 'StartTime' from 'EndTime'\n",
    "        df_machine_operating['Downtime'] = df_machine_operating['EndTime'] - df_machine_operating['StartTime']\n",
    "        \n",
    "        # Store the result as 'operations_with_downtime'\n",
    "        operations_with_downtime = df_machine_operating\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"operations_with_downtime\": operations_with_downtime}\n",
    "\n",
    "pandas_123()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows from DataFrame `df_patient_visits`, considering only 'PatientID' and 'VisitDate', saving unique rows as `unique_patient_visits`.\n",
    "@check_pandas_124\n",
    "def pandas_124(df_patient_visits):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Remove duplicates considering only 'PatientID' and 'VisitDate' columns\n",
    "        unique_patient_visits = df_patient_visits.drop_duplicates(subset=['PatientID', 'VisitDate'])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"unique_patient_visits\": unique_patient_visits}\n",
    "\n",
    "pandas_124()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Optimize data processing speed in DataFrame `df_satellite_data` by converting all text-based columns to category type, storing efficient version as `satellite_optimized`.\n",
    "@check_pandas_125\n",
    "def pandas_125(df_satellite_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Identify text-based columns (objects) and convert them to 'category' type\n",
    "        text_columns = df_satellite_data.select_dtypes(include=['object']).columns\n",
    "        df_satellite_data[text_columns] = df_satellite_data[text_columns].astype('category')\n",
    "\n",
    "        # Store the optimized DataFrame as 'satellite_optimized'\n",
    "        satellite_optimized = df_satellite_data\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"satellite_optimized\": satellite_optimized}\n",
    "\n",
    "pandas_125()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use vectorized operations on DataFrame `df_temperature_logs` to convert 'TemperatureC' to Fahrenheit, storing the result in `temperature_fahrenheit`.\n",
    "@check_pandas_126\n",
    "def pandas_126(df_temperature_logs):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Convert 'TemperatureC' to Fahrenheit using the formula F = C * 9/5 + 32\n",
    "        df_temperature_logs['TemperatureF'] = df_temperature_logs['TemperatureC'] * 9 / 5 + 32\n",
    "        \n",
    "        # Store the result in 'temperature_fahrenheit'\n",
    "        temperature_fahrenheit = df_temperature_logs['TemperatureF']\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"temperature_fahrenheit\": temperature_fahrenheit}\n",
    "\n",
    "pandas_126()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Group DataFrame `df_streaming_data` by 'UserID' and apply a lambda function to compute total view time, storing the result as `total_view_time`.\n",
    "@check_pandas_127\n",
    "def pandas_127(df_streaming_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Group by 'UserID' and apply a lambda function to compute the total view time\n",
    "        total_view_time = df_streaming_data.groupby('UserID')['ViewTime'].apply(lambda x: x.sum())\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"total_view_time\": total_view_time}\n",
    "\n",
    "pandas_127()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Use DataFrame `df_sales_analytics` to implement cohort analysis, calculating first purchase date and cohort index, storing result in `sales_cohorts`.\n",
    "@check_pandas_128\n",
    "def pandas_128(df_sales_analytics):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Ensure 'PurchaseDate' is in datetime format\n",
    "        df_sales_analytics['PurchaseDate'] = pd.to_datetime(df_sales_analytics['PurchaseDate'])\n",
    "\n",
    "        # Calculate the first purchase date for each user (cohort)\n",
    "        df_sales_analytics['FirstPurchaseDate'] = df_sales_analytics.groupby('UserID')['PurchaseDate'].transform('min')\n",
    "\n",
    "        # Calculate the cohort index: difference in months between each purchase and the first purchase\n",
    "        df_sales_analytics['CohortIndex'] = ((df_sales_analytics['PurchaseDate'].dt.year - df_sales_analytics['FirstPurchaseDate'].dt.year) * 12 +\n",
    "                                             (df_sales_analytics['PurchaseDate'].dt.month - df_sales_analytics['FirstPurchaseDate'].dt.month))\n",
    "\n",
    "        # Store the result as 'sales_cohorts'\n",
    "        sales_cohorts = df_sales_analytics[['UserID', 'FirstPurchaseDate', 'CohortIndex']]\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sales_cohorts\": sales_cohorts}\n",
    "\n",
    "pandas_128()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# In DataFrame `df_web_traffic`, apply conversion to 'Date' column from string to datetime, saving the updated DataFrame as `web_traffic_dates`.\n",
    "@check_pandas_129\n",
    "def pandas_129(df_web_traffic):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Convert the 'Date' column from string to datetime\n",
    "        df_web_traffic['Date'] = pd.to_datetime(df_web_traffic['Date'])\n",
    "\n",
    "        # Store the updated DataFrame as 'web_traffic_dates'\n",
    "        web_traffic_dates = df_web_traffic\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"web_traffic_dates\": web_traffic_dates}\n",
    "\n",
    "pandas_129()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame `df_energy_savings` with datetime index representing bi-weekly dates over one year, filled with repetitive 5% and 10% energy savings, stored in 'SavingsPercent'.\n",
    "@check_pandas_130\n",
    "def pandas_130():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    try:\n",
    "        # Generate bi-weekly date range over one year\n",
    "        date_range = pd.date_range(start='2023-01-01', periods=26, freq='2W')  # 26 bi-weekly periods in one year\n",
    "\n",
    "        # Create alternating savings percentages (5% and 10%)\n",
    "        savings_percent = np.tile([5, 10], len(date_range) // 2)\n",
    "\n",
    "        # Create DataFrame with 'Date' as index and 'SavingsPercent' as values\n",
    "        df_energy_savings = pd.DataFrame({\n",
    "            'SavingsPercent': savings_percent[:len(date_range)]  # Trim to the exact length of the date range\n",
    "        }, index=date_range)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing DataFrame: {e}\")\n",
    "    # ABOVE GOES YOUR CODE\\\n",
    "    return {\"df_energy_savings\": df_energy_savings}\n",
    "\n",
    "pandas_130()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# With DataFrame `df_vehicle_data`, apply method chaining to filter 'Type' as 'SUV' and sort by 'RecallDate', saving sorted SUVs as `sorted_suvs`.\n",
    "@check_pandas_131\n",
    "def pandas_131(df_vehicle_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    sorted_suvs = (df_vehicle_data[df_vehicle_data['Type'] == 'SUV']\n",
    "                   .sort_values(by='RecallDate'))\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sorted_suvs\": sorted_suvs}\n",
    "\n",
    "pandas_131()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Extract 'year' from 'DateAdmitted' in DataFrame `df_patient_admissions` using datetime operations, storing extracted years in `admission_years`.\n",
    "@check_pandas_132\n",
    "def pandas_132(df_patient_admissions):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    # Ensure 'DateAdmitted' is in datetime format\n",
    "    df_patient_admissions['DateAdmitted'] = pd.to_datetime(df_patient_admissions['DateAdmitted'])\n",
    "\n",
    "    # Extract the year from 'DateAdmitted' and store it in 'admission_years'\n",
    "    admission_years = df_patient_admissions['DateAdmitted'].dt.year\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"admission_years\": admission_years}\n",
    "\n",
    "pandas_132()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Process DataFrame `df_cost_analysis` by converting 'Amount' to USD using 'Currency' which contains the type like ['EUR', 'GBP'] conversion rates being EUR: 1.12 and GBP: 1.30, storing result as `cost_analysis_usd` with column 'Amount' in 'Currency' and 'AmountUSD' in USD.\n",
    "@check_pandas_133\n",
    "def pandas_133(df_cost_analysis):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    # Define conversion rates\n",
    "    conversion_rates = {'EUR': 1.12, 'GBP': 1.30}\n",
    "\n",
    "    # Apply conversion based on 'Currency'\n",
    "    df_cost_analysis['AmountUSD'] = df_cost_analysis.apply(\n",
    "        lambda row: row['Amount'] * conversion_rates.get(row['Currency'], 1) if row['Currency'] in conversion_rates else row['Amount'], axis=1)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"cost_analysis_usd\": cost_analysis_usd}\n",
    "\n",
    "pandas_133()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# In DataFrame `df_logistics_data`, use rolling window of 30 days to calculate moving average inventory level, storing as `thirty_day_moving_inventory`.\n",
    "@check_pandas_134\n",
    "def pandas_134(df_logistics_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    thirty_day_moving_inventory = df_logistics_data['InventoryLevel'].rolling(window=30).mean()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"thirty_day_moving_inventory\": thirty_day_moving_inventory}\n",
    "\n",
    "pandas_134()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# From DataFrame `df_sports_results`, extract top three teams based on 'Scores', sorting first, saving top teams as `top_teams`.\n",
    "@check_pandas_135\n",
    "def pandas_135(df_sports_results):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    top_teams = df_sports_results.sort_values(by='Scores', ascending=False).head(3)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"top_teams\": top_teams}\n",
    "\n",
    "pandas_135()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Use DataFrame `df_network_activity` statistics to calculate z-scores for 'Bandwidth', storing in column 'ZscoreBandwidth' as `network_with_zscore`.\n",
    "@check_pandas_136\n",
    "def pandas_136(df_network_activity):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    # Calculate the mean and standard deviation for 'Bandwidth'\n",
    "    mean_bandwidth = df_network_activity['Bandwidth'].mean()\n",
    "    std_bandwidth = df_network_activity['Bandwidth'].std()\n",
    "\n",
    "    # Calculate z-scores for 'Bandwidth' and store it in 'ZscoreBandwidth'\n",
    "    df_network_activity['ZscoreBandwidth'] = (df_network_activity['Bandwidth'] - mean_bandwidth) / std_bandwidth\n",
    "\n",
    "    # Assign the modified DataFrame to the result variable\n",
    "    network_with_zscore = df_network_activity\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"network_with_zscore\": network_with_zscore}\n",
    "\n",
    "pandas_136()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Perform inplace boolean update of `df_market_responses` setting all 'ResponseTime' > 300 to 300, saving updated DataFrame.\n",
    "@check_pandas_137\n",
    "def pandas_137(df_market_responses):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    df_market_responses.loc[df_market_responses['ResponseTime'] > 300, 'ResponseTime'] = 300\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"df_market_responses\": df_market_responses}\n",
    "\n",
    "pandas_137()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Analyze DataFrame `df_social_behaviors` applying transform with a custom function to 'EngagementRate' to normalize within each 'Group' by z-score, storing as 'NormalizedEngagementRate'.\n",
    "@check_pandas_138\n",
    "def pandas_138(df_social_behaviors):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    # Define a function to calculate z-score\n",
    "    def z_score(x):\n",
    "        return (x - x.mean()) / x.std()\n",
    "\n",
    "    # Apply transform with z_score function to 'EngagementRate' grouped by 'Group'\n",
    "    df_social_behaviors['NormalizedEngagementRate'] = df_social_behaviors.groupby('Group')['EngagementRate'].transform(z_score)\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"df_social_behaviors\": df_social_behaviors}\n",
    "\n",
    "pandas_138()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Use DataFrame `df_product_launch` to derive 'ResponseRate' from 'Inquiries' divided by 'Sales', multiplied by 100, storing as `launch_responses`.\n",
    "@check_pandas_139\n",
    "def pandas_139(df_product_launch):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    df_product_launch['ResponseRate'] = (df_product_launch['Inquiries'] / df_product_launch['Sales']) * 100\n",
    "    launch_responses = df_product_launch['ResponseRate']\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"launch_responses\": launch_responses}\n",
    "\n",
    "pandas_139()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `create_series_from_dict` that takes a dictionary `input_dict` as an argument, and returns a pandas Series with the dictionary keys as the Series index.\n",
    "@check_pandas_140\n",
    "def pandas_3():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def create_series_from_dict(input_dict):\n",
    "        return pd.Series(input_dict)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"create_series_from_dict\": create_series_from_dict}\n",
    "\n",
    "pandas_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define a function `filter_series_positive` that takes a pandas Series `data_series` and returns a new Series containing only the elements where the value is positive.\n",
    "@check_pandas_141\n",
    "def pandas_4():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def filter_series_positive(data_series):\n",
    "        return data_series[data_series > 0]\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"filter_series_positive\": filter_series_positive}\n",
    "\n",
    "pandas_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Develop a function `rename_dataframe_columns` that accepts a DataFrame `df` and a dictionary `column_mapping`, and returns the DataFrame with renamed columns according to the mapping.\n",
    "@check_pandas_142\n",
    "def pandas_142():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def rename_dataframe_columns(df, column_mapping):\n",
    "        return df.rename(columns=column_mapping)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"rename_dataframe_columns\": rename_dataframe_columns}\n",
    "\n",
    "pandas_142()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `drop_nan_rows` that takes a DataFrame `input_df` and returns a new DataFrame with all rows containing any NaN values removed.\n",
    "@check_pandas_143\n",
    "def pandas_143():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def drop_nan_rows(input_df):\n",
    "        # Drop rows with any NaN values and return the resulting DataFrame\n",
    "        return input_df.dropna()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"drop_nan_rows\": drop_nan_rows}\n",
    "\n",
    "pandas_143()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Construct a function `fill_missing_with_mean` which takes a DataFrame `df` and fills missing values in each numeric column with the mean of that column, returning the modified DataFrame.\n",
    "@check_pandas_144\n",
    "def pandas_144():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def fill_missing_with_mean(df):\n",
    "        # Iterate through each column in the DataFrame\n",
    "        for column in df.select_dtypes(include='number').columns:\n",
    "            # Fill missing values in each numeric column with the mean of that column\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"fill_missing_with_mean\": fill_missing_with_mean}\n",
    "\n",
    "pandas_144()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Implement a function `calculate_group_means` that takes a DataFrame `df` and a column name `group_col`, grouping by `group_col`, then returning the mean of each group as a Series.\n",
    "@check_pandas_145\n",
    "def pandas_145():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_group_means(df, group_col):\n",
    "        # Group the DataFrame by the specified column and calculate the mean of each group\n",
    "        return df.groupby(group_col).mean()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_group_means\": calculate_group_means}\n",
    "\n",
    "pandas_145()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Design a function `subset_dataframe_label` that accepts a DataFrame `df`, a list `rows` of row labels, and returns a subset DataFrame containing only those rows.\n",
    "@check_pandas_146\n",
    "def pandas_146():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def subset_dataframe_label(df, rows):\n",
    "        # Subset the DataFrame based on the row labels provided in the 'rows' list\n",
    "        return df.loc[rows]\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"subset_dataframe_label\": subset_dataframe_label}\n",
    "\n",
    "pandas_146()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define a function `merge_dataframes_on_key` that takes two DataFrames `df1`, `df2`, and a string `key`, merging them on the shared key column and returning the merged DataFrame.\n",
    "@check_pandas_147\n",
    "def pandas_147():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def merge_dataframes_on_key(df1, df2, key):\n",
    "        # Perform the merge operation on the given key column\n",
    "        return pd.merge(df1, df2, on=key)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"merge_dataframes_on_key\": merge_dataframes_on_key}\n",
    "\n",
    "pandas_147()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `aggregate_sales_by_region` which accepts a DataFrame `sales_df` that includes columns 'Region' and 'Sales', and returns a DataFrame with the total sales per region.\n",
    "@check_pandas_148\n",
    "def pandas_148():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def aggregate_sales_by_region(sales_df):\n",
    "        # Group by 'Region' and calculate the sum of 'Sales' for each region\n",
    "        return sales_df.groupby('Region')['Sales'].sum().reset_index()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"aggregate_sales_by_region\": aggregate_sales_by_region}\n",
    "\n",
    "pandas_148()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Implement a function `pivot_table_for_analysis` that takes a DataFrame `df` and strings `index`, `columns`, `values`, and returns a pivot table using those specifications.\n",
    "@check_pandas_149\n",
    "def pandas_149():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def pivot_table_for_analysis(df, index, columns, values):\n",
    "        # Create a pivot table using the given parameters\n",
    "        return pd.pivot_table(df, index=index, columns=columns, values=values)\n",
    "    \n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"pivot_table_for_analysis\": pivot_table_for_analysis}\n",
    "\n",
    "pandas_149()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `calculate_rolling_average` that accepts a DataFrame `df` and an integer `window` to compute the rolling average of a numeric column `column_name`, returning the updated series.\n",
    "@check_pandas_150\n",
    "def pandas_150():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_rolling_average(df, column_name, window):\n",
    "        # Compute the rolling average for the specified column with the given window size\n",
    "        return df[column_name].rolling(window=window).mean()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_rolling_average\": calculate_rolling_average}\n",
    "\n",
    "pandas_150()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Construct a function `resample_time_series` which takes a time-indexed DataFrame `time_df` and a frequency string `freq`, resampling the DataFrame to the new frequency and summing, returning the result.\n",
    "@check_pandas_151\n",
    "def pandas_151():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def resample_time_series(time_df, freq):\n",
    "        # Resample the time-indexed DataFrame to the given frequency and sum the values\n",
    "        return time_df.resample(freq).sum()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"resample_time_series\": resample_time_series}\n",
    "\n",
    "pandas_151()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `expand_string_columns` that takes a DataFrame `df` and a list of columns `string_columns`, expanding each string column to lowercase, and returns the modified DataFrame.\n",
    "@check_pandas_152\n",
    "def pandas_152():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def expand_string_columns(df, string_columns):\n",
    "        for col in string_columns:\n",
    "            if col in df.columns and df[col].dtype == 'object':  # Check if column exists and is of type string\n",
    "                df[col] = df[col].str.lower()  # Convert to lowercase\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"expand_string_columns\": expand_string_columns}\n",
    "\n",
    "pandas_152()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Design a function `convert_to_datetime_index` that accepts a DataFrame `df` and a column name `date_col`, converting it to a DateTimeIndex, and returning the updated DataFrame.\n",
    "@check_pandas_153\n",
    "def pandas_153():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def convert_to_datetime_index(df, date_col):\n",
    "        df[date_col] = pd.to_datetime(df[date_col])  # Convert the specified column to datetime\n",
    "        df.set_index(date_col, inplace=True)  # Set the datetime column as the index\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"convert_to_datetime_index\": convert_to_datetime_index}\n",
    "\n",
    "pandas_153()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Implement a function `identify_and_remove_outliers` that identifies outliers in a Series `data_series` using a specified `threshold` by removing the values outside of the `threshodl` multiplied by the standard deviation from the mean.\n",
    "@check_pandas_154\n",
    "def pandas_154():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def identify_and_remove_outliers(data_series, threshold):\n",
    "        mean = data_series.mean()\n",
    "        std_dev = data_series.std()\n",
    "        \n",
    "        # Calculate the upper and lower bounds based on the threshold and standard deviation\n",
    "        lower_bound = mean - threshold * std_dev\n",
    "        upper_bound = mean + threshold * std_dev\n",
    "        \n",
    "        # Filter the series to remove outliers\n",
    "        filtered_series = data_series[(data_series >= lower_bound) & (data_series <= upper_bound)]\n",
    "        \n",
    "        return filtered_series\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"identify_and_remove_outliers\": identify_and_remove_outliers}\n",
    "\n",
    "pandas_154()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `calculate_percentage_change` which computes the percentage change over time for a Series `time_series` and returns the updated Series with NaN for the first entry.\n",
    "@check_pandas_155\n",
    "def pandas_155():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_percentage_change(time_series):\n",
    "        # Calculate the percentage change using pandas' pct_change method\n",
    "        percentage_change = time_series.pct_change() * 100\n",
    "        \n",
    "        # Set the first entry to NaN (already done by pct_change, but reinforcing it)\n",
    "        percentage_change.iloc[0] = float('nan')\n",
    "        \n",
    "        return percentage_change\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_percentage_change\": calculate_percentage_change}\n",
    "\n",
    "pandas_155()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Define a function `categorize_based_on_values` which takes a DataFrame `df`, a column `categorical_col`, and returns a DataFrame with additional column 'Category' based on ranges in `categorical_col` with labels ['Low', 'Medium', 'High'] and bins [0, 100, 200, 300].\n",
    "@check_pandas_156\n",
    "def pandas_156():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def categorize_based_on_values(df, categorical_col):\n",
    "        # Define the bins and labels\n",
    "        bins = [0, 100, 200, 300]\n",
    "        labels = ['Low', 'Medium', 'High']\n",
    "        \n",
    "        # Create a new column 'Category' based on the specified bins and labels\n",
    "        df['Category'] = pd.cut(df[categorical_col], bins=bins, labels=labels, right=False)\n",
    "        \n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"categorize_based_on_values\": categorize_based_on_values}\n",
    "\n",
    "pandas_156()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `shift_dataframe_rows` that takes a DataFrame `df` and an integer `periods`, shifting all rows by the specified number of periods and returning the DataFrame.\n",
    "@check_pandas_157\n",
    "def pandas_157():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def shift_dataframe_rows(df, periods):\n",
    "        # Shift all rows by the specified number of periods\n",
    "        return df.shift(periods)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"shift_dataframe_rows\": shift_dataframe_rows}\n",
    "\n",
    "pandas_157()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Develop a function `aggregate_and_flatten_grouped` that takes a grouped DataFrame `group_df` and returns a flattened DataFrame with 'sum' and 'count' aggregation for each group and reseted index.\n",
    "@check_pandas_158\n",
    "def pandas_158():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def aggregate_and_flatten_grouped(group_df):\n",
    "        # Apply aggregation: sum and count for each group\n",
    "        aggregated_df = group_df.aggregate(['sum', 'count'])\n",
    "        \n",
    "        # Flatten the columns to a single level\n",
    "        aggregated_df.columns = [f'{col}_{agg}' for col, agg in aggregated_df.columns]\n",
    "        \n",
    "        # Reset the index to flatten the DataFrame\n",
    "        aggregated_df.reset_index(inplace=True)\n",
    "        \n",
    "        return aggregated_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"aggregate_and_flatten_grouped\": aggregate_and_flatten_grouped}\n",
    "\n",
    "pandas_158()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Construct a function named `remove_duplicates_by_columns` that takes a DataFrame `df` and a list `subset_columns`, and removes duplicate rows based on these columns, returning the resulting DataFrame.\n",
    "@check_pandas_159\n",
    "def pandas_159():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def remove_duplicates_by_columns(df, subset_columns):\n",
    "        # Remove duplicates based on the specified subset of columns\n",
    "        return df.drop_duplicates(subset=subset_columns)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"remove_duplicates_by_columns\": remove_duplicates_by_columns}\n",
    "\n",
    "pandas_159()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Design a function `calculate_memory_usage` that takes a DataFrame `df` and returns the total memory usage in MB, both with and without optimizations for all columns possible.\n",
    "@check_pandas_160\n",
    "def pandas_160():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_memory_usage(df):\n",
    "        # Calculate memory usage without optimization (in bytes)\n",
    "        memory_before = df.memory_usage(deep=True).sum() / 1024**2  # Convert to MB\n",
    "        \n",
    "        # Optimize memory usage by changing data types\n",
    "        optimized_df = df.copy()\n",
    "        \n",
    "        for col in optimized_df.select_dtypes(include=['float64']).columns:\n",
    "            # Convert float64 to float32 if possible\n",
    "            optimized_df[col] = optimized_df[col].astype('float32')\n",
    "        \n",
    "        for col in optimized_df.select_dtypes(include=['int64']).columns:\n",
    "            # Convert int64 to int32 or int16 if possible\n",
    "            if optimized_df[col].min() >= -2**31 and optimized_df[col].max() <= 2**31 - 1:\n",
    "                optimized_df[col] = optimized_df[col].astype('int32')\n",
    "            elif optimized_df[col].min() >= -2**15 and optimized_df[col].max() <= 2**15 - 1:\n",
    "                optimized_df[col] = optimized_df[col].astype('int16')\n",
    "\n",
    "        for col in optimized_df.select_dtypes(include=['object']).columns:\n",
    "            # Convert object columns to category if there are fewer unique values\n",
    "            num_unique = len(optimized_df[col].unique())\n",
    "            num_total = len(optimized_df[col])\n",
    "            if num_unique / num_total < 0.5:\n",
    "                optimized_df[col] = optimized_df[col].astype('category')\n",
    "        \n",
    "        # Calculate memory usage with optimization (in MB)\n",
    "        memory_after = optimized_df.memory_usage(deep=True).sum() / 1024**2  # Convert to MB\n",
    "        \n",
    "        return {\"memory_before\": memory_before, \"memory_after\": memory_after}\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_memory_usage\": calculate_memory_usage}\n",
    "\n",
    "pandas_160()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Implement a function `map_values_with_dict` which accepts a DataFrame `df`, a column `target_col`, and a dictionary `value_map`, returning the updated DataFrame after mapping.\n",
    "@check_pandas_161\n",
    "def pandas_161():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def map_values_with_dict(df, target_col, value_map):\n",
    "        # Map the values in the target column using the provided dictionary\n",
    "        df[target_col] = df[target_col].map(value_map)\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"map_values_with_dict\": map_values_with_dict}\n",
    "\n",
    "pandas_161()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `select_top_n_rows_based_on_column` that takes a DataFrame `df`, a column `target_col`, and an integer `n`, and returns the top `n` rows sorted by `target_col`.\n",
    "@check_pandas_162\n",
    "def pandas_162():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def select_top_n_rows_based_on_column(df, target_col, n):\n",
    "        # Sort the DataFrame by the target column in descending order and select the top n rows\n",
    "        sorted_df = df.sort_values(by=target_col, ascending=False)\n",
    "        return sorted_df.head(n)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"select_top_n_rows_based_on_column\": select_top_n_rows_based_on_column}\n",
    "\n",
    "pandas_162()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `replace_substrings_in_column` that takes a DataFrame `df`, a column `text_col`, a string `old`, and a string `new`, replacing all occurrences of `old` with `new` in `text_col`.\n",
    "@check_pandas_163\n",
    "def pandas_163():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def replace_substrings_in_column(df, text_col, old, new):\n",
    "        # Replace all occurrences of 'old' with 'new' in the specified text column\n",
    "        df[text_col] = df[text_col].str.replace(old, new, regex=False)\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"replace_substrings_in_column\": replace_substrings_in_column}\n",
    "\n",
    "pandas_163()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Define a function `apply_discretization_binner` which bins Series `data_series` into a specified number of discrete intervals `n_bins` and returns the binned Series.\n",
    "@check_pandas_164\n",
    "def pandas_164():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def apply_discretization_binner(data_series, n_bins):\n",
    "        # Bin the data into 'n_bins' discrete intervals\n",
    "        binned_series = pd.cut(data_series, bins=n_bins, labels=False)\n",
    "        return binned_series\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"apply_discretization_binner\": apply_discretization_binner}\n",
    "\n",
    "pandas_164()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `generate_descriptive_statistics` that takes a DataFrame `df` and returns a DataFrame with descriptive statistics such as mean, median, and standard deviation for all numeric columns.\n",
    "@check_pandas_165\n",
    "def pandas_165():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def generate_descriptive_statistics(df):\n",
    "        # Filter numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=['number'])\n",
    "\n",
    "        # Calculate statistics\n",
    "        stats_df = pd.DataFrame({\n",
    "            'Mean': numeric_cols.mean(),\n",
    "            'Median': numeric_cols.median(),\n",
    "            'Standard Deviation': numeric_cols.std()\n",
    "        })\n",
    "        return stats_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"generate_descriptive_statistics\": generate_descriptive_statistics}\n",
    "\n",
    "pandas_165()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Implement a function `convert_column_dtype` that accepts a DataFrame `df`, a column name `col`, and a data type `new_type`, and returns the DataFrame with `col` converted to `new_type`.\n",
    "@check_pandas_166\n",
    "def pandas_166():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def convert_column_dtype(df, col, new_type):\n",
    "        # Convert the specified column to the new data type\n",
    "        df[col] = df[col].astype(new_type)\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"convert_column_dtype\": convert_column_dtype}\n",
    "\n",
    "pandas_166()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Develop a function `sort_dataframe_by_multiple_columns` taking DataFrame `df` and a list `columns_list` to sort the DataFrame by these columns, returning the sorted DataFrame.\n",
    "@check_pandas_167\n",
    "def pandas_167():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def sort_dataframe_by_multiple_columns(df, columns_list):\n",
    "        # Sort the DataFrame by the specified columns\n",
    "        return df.sort_values(by=columns_list)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sort_dataframe_by_multiple_columns\": sort_dataframe_by_multiple_columns}\n",
    "\n",
    "pandas_167()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `create_time_based_features` which accepts a DataFrame `time_df` with a 'Timestamp' column and returns the DataFrame with new columns for hour, day, and month.\n",
    "@check_pandas_168\n",
    "def pandas_168():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def create_time_based_features(time_df):\n",
    "        # Ensure 'Timestamp' is in datetime format\n",
    "        time_df['Timestamp'] = pd.to_datetime(time_df['Timestamp'])\n",
    "\n",
    "        # Create new columns for hour, day, and month\n",
    "        time_df['Hour'] = time_df['Timestamp'].dt.hour\n",
    "        time_df['Day'] = time_df['Timestamp'].dt.day\n",
    "        time_df['Month'] = time_df['Timestamp'].dt.month\n",
    "\n",
    "        return time_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"create_time_based_features\": create_time_based_features}\n",
    "\n",
    "pandas_168()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `analyze_sales_data` that takes a DataFrame `sales_df` with columns 'Date', 'Region', and 'Sales'. The function should: Convert 'Date' to a DateTime object; Filter out any rows where 'Sales' is negative; Group by 'Region' to calculate the total and average sales; Return a DataFrame with columns 'Region', 'TotalSales', 'AverageSales';\n",
    "@check_pandas_169\n",
    "def pandas_169():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def analyze_sales_data(sales_df):\n",
    "        # Convert 'Date' column to datetime\n",
    "        sales_df['Date'] = pd.to_datetime(sales_df['Date'])\n",
    "\n",
    "        # Filter out rows where 'Sales' is negative\n",
    "        sales_df = sales_df[sales_df['Sales'] >= 0]\n",
    "\n",
    "        # Group by 'Region' and calculate total and average sales\n",
    "        grouped = sales_df.groupby('Region')['Sales']\n",
    "        total_sales = grouped.sum()\n",
    "        average_sales = grouped.mean()\n",
    "\n",
    "        # Create the resulting DataFrame\n",
    "        result = pd.DataFrame({\n",
    "            'Region': total_sales.index,\n",
    "            'TotalSales': total_sales.values,\n",
    "            'AverageSales': average_sales.values\n",
    "        })\n",
    "\n",
    "        return result\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"analyze_sales_data\": analyze_sales_data}\n",
    "\n",
    "pandas_169()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `clean_and_merge_datasets` that takes two DataFrames `df_left` and `df_right`, and: Fills NA values in `df_left` with 0; Drops any duplicate rows in `df_right`; Merges the cleaned DataFrames on a common column 'Key', using an outer join; Returns the merged DataFrame sorted by 'Key';\n",
    "@check_pandas_170\n",
    "def pandas_170():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def clean_and_merge_datasets(df_left, df_right):\n",
    "        # Fill NA values in df_left with 0\n",
    "        df_left_cleaned = df_left.fillna(0)\n",
    "\n",
    "        # Drop duplicate rows in df_right\n",
    "        df_right_cleaned = df_right.drop_duplicates()\n",
    "\n",
    "        # Merge the cleaned DataFrames on 'Key' using an outer join\n",
    "        merged_df = pd.merge(\n",
    "            df_left_cleaned, df_right_cleaned, on='Key', how='outer'\n",
    "        )\n",
    "\n",
    "        # Sort the merged DataFrame by 'Key'\n",
    "        merged_df = merged_df.sort_values(by='Key')\n",
    "\n",
    "        return merged_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"clean_and_merge_datasets\": clean_and_merge_datasets}\n",
    "\n",
    "pandas_170()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Develop a function `process_sensor_data_batch` that receives a DataFrame `sensor_df` containing 'SensorID', 'ReadingValue', 'Timestamp'. Convert 'Timestamp' to datetime format; Ensure all 'ReadingValue' entries are non-negative; Calculate the mean and standard deviation of 'ReadingValue' for each 'SensorID'; Return a DataFrame with 'SensorID', 'MeanReading', 'StdDevReading';\n",
    "@check_pandas_171\n",
    "def pandas_171():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def process_sensor_data_batch(sensor_df):\n",
    "        # Convert 'Timestamp' to datetime format\n",
    "        sensor_df['Timestamp'] = pd.to_datetime(sensor_df['Timestamp'])\n",
    "\n",
    "        # Ensure all 'ReadingValue' entries are non-negative\n",
    "        sensor_df = sensor_df[sensor_df['ReadingValue'] >= 0]\n",
    "\n",
    "        # Group by 'SensorID' and calculate mean and standard deviation\n",
    "        stats = sensor_df.groupby('SensorID')['ReadingValue'].agg(\n",
    "            MeanReading='mean', StdDevReading='std'\n",
    "        ).reset_index()\n",
    "\n",
    "        return stats\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"process_sensor_data_batch\": process_sensor_data_batch}\n",
    "\n",
    "pandas_171()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Construct a function `analyze_customer_behaviors` to handle a DataFrame `customer_df` with columns 'CustomerID', 'PurchaseAmount', 'VisitTimestamp'. Convert 'VisitTimestamp' to a DateTimeIndex; Filter to include only purchases greater than a specified `min_purchase`; Group by 'CustomerID' to determine the total and count of purchases; Return a DataFrame with 'CustomerID', 'TotalPurchases', 'NumberVisits', and attach the most recent visit date;\n",
    "@check_pandas_172\n",
    "def pandas_172():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def analyze_customer_behaviors(customer_df, min_purchase):\n",
    "        # Convert 'VisitTimestamp' to datetime\n",
    "        customer_df['VisitTimestamp'] = pd.to_datetime(customer_df['VisitTimestamp'])\n",
    "\n",
    "        # Filter rows based on 'min_purchase'\n",
    "        filtered_df = customer_df[customer_df['PurchaseAmount'] > min_purchase]\n",
    "\n",
    "        # Group by 'CustomerID' to compute aggregates\n",
    "        grouped = filtered_df.groupby('CustomerID').agg(\n",
    "            TotalPurchases=('PurchaseAmount', 'sum'),\n",
    "            NumberVisits=('PurchaseAmount', 'count'),\n",
    "            MostRecentVisit=('VisitTimestamp', 'max')\n",
    "        ).reset_index()\n",
    "\n",
    "        return grouped\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"analyze_customer_behaviors\": analyze_customer_behaviors}\n",
    "\n",
    "pandas_172()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `transform_financial_data` that processes a DataFrame `financial_df` including columns 'AccountID', 'TransactionDate', 'Amount'. Parse 'TransactionDate' into datetime and set as index; Filter 'Amount' to exclude NaN and zero values; Extract month and year from 'TransactionDate' into new columns `Month`, `Year`; Group by 'AccountID' and 'Year' to summarize monthly 'Amount' into sum and mean, returning a structured DataFrame;\n",
    "@check_pandas_173\n",
    "def pandas_173():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def transform_financial_data(financial_df):\n",
    "        # Parse 'TransactionDate' to datetime and set as index\n",
    "        financial_df['TransactionDate'] = pd.to_datetime(financial_df['TransactionDate'])\n",
    "        financial_df.set_index('TransactionDate', inplace=True)\n",
    "\n",
    "        # Filter out NaN and zero values in 'Amount'\n",
    "        filtered_df = financial_df[financial_df['Amount'].notna() & (financial_df['Amount'] != 0)]\n",
    "\n",
    "        # Use .loc to add new columns explicitly\n",
    "        filtered_df.loc[:, 'Month'] = filtered_df.index.month\n",
    "        filtered_df.loc[:, 'Year'] = filtered_df.index.year\n",
    "\n",
    "        # Group by 'AccountID' and 'Year' to calculate sum and mean of 'Amount'\n",
    "        grouped = (\n",
    "            filtered_df\n",
    "            .groupby(['AccountID', 'Year', 'Month'])\n",
    "            .agg(\n",
    "                TotalAmount=('Amount', 'sum'),\n",
    "                AverageAmount=('Amount', 'mean')\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        return grouped\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"transform_financial_data\": transform_financial_data}\n",
    "\n",
    "pandas_173()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Implement a function `aggregate_weather_data` for a DataFrame `weather_df` with fields 'StationID', 'Temp', 'Humidity', 'ObservationTime'. Convert 'ObservationTime' to a DateTimeIndex; Filter to retain records with positive 'Temp' and 'Humidity'; Resample to daily frequency, taking the mean for each day; Return a DataFrame grouped by 'StationID' with columns for daily average 'Temp' and 'Humidity';\n",
    "@check_pandas_174\n",
    "def pandas_174():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def aggregate_weather_data(weather_df):\n",
    "        import pandas as pd\n",
    "\n",
    "        # Convert 'ObservationTime' to datetime and set as index\n",
    "        weather_df['ObservationTime'] = pd.to_datetime(weather_df['ObservationTime'])\n",
    "        weather_df.set_index('ObservationTime', inplace=True)\n",
    "\n",
    "        # Filter to retain records with positive 'Temp' and 'Humidity'\n",
    "        filtered_df = weather_df[(weather_df['Temp'] > 0) & (weather_df['Humidity'] > 0)]\n",
    "\n",
    "        # Resample to daily frequency and calculate the mean for each day\n",
    "        daily_resampled = filtered_df.resample('D').mean()\n",
    "\n",
    "        # Group by 'StationID' and calculate daily average 'Temp' and 'Humidity'\n",
    "        grouped = (\n",
    "            daily_resampled\n",
    "            .groupby('StationID')\n",
    "            .agg(\n",
    "                DailyAvgTemp=('Temp', 'mean'),\n",
    "                DailyAvgHumidity=('Humidity', 'mean')\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        return grouped\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"aggregate_weather_data\": aggregate_weather_data}\n",
    "\n",
    "pandas_174()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Design a function `standardize_student_record` to clean a DataFrame `student_df` featuring 'Name', 'Score', 'SubmissionDate'. Standardize 'Name' to have a capitalized first letter; Address missing 'Score' values by assigning the median score; Standardize 'SubmissionDate' to a consistent format and compute 'DaysSinceSubmission'; Return a DataFrame with standardized names, computed days since submission, and filled scores;\n",
    "@check_pandas_175\n",
    "def pandas_175():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def standardize_student_record(student_df):\n",
    "        import pandas as pd\n",
    "        from datetime import datetime\n",
    "\n",
    "        # Standardize 'Name' to have a capitalized first letter\n",
    "        student_df['Name'] = student_df['Name'].str.title()\n",
    "\n",
    "        # Fill missing 'Score' values with the median score\n",
    "        median_score = student_df['Score'].median()\n",
    "        student_df['Score'].fillna(median_score, inplace=True)\n",
    "\n",
    "        # Standardize 'SubmissionDate' to a consistent format and compute 'DaysSinceSubmission'\n",
    "        student_df['SubmissionDate'] = pd.to_datetime(student_df['SubmissionDate'], errors='coerce')\n",
    "        today = pd.Timestamp.now()\n",
    "        student_df['DaysSinceSubmission'] = (today - student_df['SubmissionDate']).dt.days\n",
    "\n",
    "        # Return the cleaned and standardized DataFrame\n",
    "        return student_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"standardize_student_record\": standardize_student_record}\n",
    "\n",
    "pandas_175()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `construct_inventory_report` that takes a DataFrame `inventory_df` with 'ItemID', 'Quantity', 'RestockDate'. Parse 'RestockDate' to ensure it's in datetime format; Identify items needing restock by checking 'Quantity' against the `threshold`; Provide a summary count of items needing restock by month; Return a detailed DataFrame with 'ItemID', 'Quantity', 'DaysUntilRestock', plus a monthly summary DataFrame;\n",
    "@check_pandas_176\n",
    "def pandas_176():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def construct_inventory_report(inventory_df, threshold):\n",
    "        import pandas as pd\n",
    "\n",
    "        # Parse 'RestockDate' to datetime format\n",
    "        inventory_df['RestockDate'] = pd.to_datetime(inventory_df['RestockDate'], errors='coerce')\n",
    "\n",
    "        # Calculate days until restock\n",
    "        today = pd.Timestamp.now()\n",
    "        inventory_df['DaysUntilRestock'] = (inventory_df['RestockDate'] - today).dt.days\n",
    "\n",
    "        # Identify items needing restock\n",
    "        inventory_df['NeedsRestock'] = inventory_df['Quantity'] < threshold\n",
    "\n",
    "        # Create a detailed report\n",
    "        detailed_report = inventory_df[['ItemID', 'Quantity', 'DaysUntilRestock', 'NeedsRestock']]\n",
    "\n",
    "        # Provide a summary count of items needing restock by month\n",
    "        inventory_df['RestockMonth'] = inventory_df['RestockDate'].dt.to_period('M')\n",
    "        monthly_summary = inventory_df[inventory_df['NeedsRestock']].groupby('RestockMonth').size().reset_index(name='ItemsNeedingRestock')\n",
    "\n",
    "        # Return detailed DataFrame and summary DataFrame\n",
    "        return detailed_report, monthly_summary\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"construct_inventory_report\": construct_inventory_report}\n",
    "\n",
    "pandas_176()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Develop a function `optimize_sales_forecast` that works on DataFrame `forecast_df` with columns 'Product', 'ProjectedSales', 'ForecastDate'. Convert 'ForecastDate' to DateTime format; Apply forward fill to handle missing 'ProjectedSales' values; Conduct a rolling window analysis to compute the 3-month moving average of sales; Return an extended DataFrame with moving averages along with original columns;\n",
    "@check_pandas_177\n",
    "def pandas_177():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def optimize_sales_forecast(forecast_df):\n",
    "        # Convert 'ForecastDate' to DateTime format\n",
    "        forecast_df['ForecastDate'] = pd.to_datetime(forecast_df['ForecastDate'])\n",
    "\n",
    "        # Apply forward fill to handle missing 'ProjectedSales' values\n",
    "        forecast_df['ProjectedSales'] = forecast_df['ProjectedSales'].ffill()\n",
    "\n",
    "        # Conduct a rolling window analysis for 3-month moving average\n",
    "        forecast_df['3_month_moving_avg'] = forecast_df.groupby('Product')['ProjectedSales'].rolling(window=3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "        return forecast_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"optimize_sales_forecast\": optimize_sales_forecast}\n",
    "\n",
    "pandas_177()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `summarize_employee_performance` that processes DataFrame `performance_df` with 'EmployeeID', 'Score', 'ReviewDate'. Ensure 'ReviewDate' is converted into a datetime object; Replace missing 'Score' with the lowest non-zero score; Group by 'EmployeeID' to compute total and average score; Generate a DataFrame highlighting top performers with scores above a specified percentile score; Audit history, including evaluation of performance improvement over time;\n",
    "@check_pandas_178\n",
    "def pandas_178():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def summarize_employee_performance(performance_df, percentile=90):\n",
    "            # Convert 'ReviewDate' to datetime\n",
    "            performance_df['ReviewDate'] = pd.to_datetime(performance_df['ReviewDate'])\n",
    "\n",
    "            # Replace missing 'Score' with the lowest non-zero score\n",
    "            min_non_zero_score = performance_df[performance_df['Score'] > 0]['Score'].min()\n",
    "            performance_df['Score'].fillna(min_non_zero_score, inplace=True)\n",
    "\n",
    "            # Group by 'EmployeeID' to compute total and average score\n",
    "            summary_df = performance_df.groupby('EmployeeID').agg(\n",
    "                total_score=('Score', 'sum'),\n",
    "                avg_score=('Score', 'mean')\n",
    "            ).reset_index()\n",
    "\n",
    "            # Generate top performers based on the specified percentile\n",
    "            threshold_score = summary_df['avg_score'].quantile(percentile / 100)\n",
    "            top_performers = summary_df[summary_df['avg_score'] > threshold_score]\n",
    "\n",
    "            # Create audit history of performance improvement over time\n",
    "            performance_df['Score_improvement'] = performance_df.groupby('EmployeeID')['Score'].diff().fillna(0)\n",
    "\n",
    "            return summary_df, top_performers, performance_df[['EmployeeID', 'ReviewDate', 'Score', 'Score_improvement']]\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"summarize_employee_performance\": summarize_employee_performance}\n",
    "\n",
    "pandas_178()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `double_series_values` that takes a Series `input_series` and returns a Series with all values doubled.\n",
    "@check_pandas_179\n",
    "def pandas_179():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def double_series_values(input_series):\n",
    "        return input_series * 2\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"double_series_values\": double_series_values}\n",
    "\n",
    "pandas_179()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `replace_zeros_with_mean` that takes a Series `data_series` and replaces all 0 values with the mean of the Series, returning the modified Series.\n",
    "@check_pandas_180\n",
    "def pandas_180():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def replace_zeros_with_mean(data_series):\n",
    "        mean_value = data_series[data_series != 0].mean()\n",
    "        return data_series.replace(0, mean_value)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"replace_zeros_with_mean\": replace_zeros_with_mean}\n",
    "\n",
    "pandas_180()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Design a function `standardize_column_names` that accepts a DataFrame `df` and returns it with all column names set to lowercase.\n",
    "@check_pandas_181\n",
    "def pandas_181():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def standardize_column_names(df):\n",
    "        df.columns = df.columns.str.lower()\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"standardize_column_names\": standardize_column_names}\n",
    "\n",
    "pandas_181()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Implement a function `drop_duplicate_rows` that receives a DataFrame `df` and returns it with duplicate rows removed.\n",
    "@check_pandas_182\n",
    "def pandas_182():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def drop_duplicate_rows(df):\n",
    "        return df.drop_duplicates()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"drop_duplicate_rows\": drop_duplicate_rows}\n",
    "\n",
    "pandas_182()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `calculate_column_sums` that takes a DataFrame `df` and returns a Series containing the sum of each column.\n",
    "@check_pandas_183\n",
    "def pandas_183():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_column_sums(df):\n",
    "        return df.sum()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_column_sums\": calculate_column_sums}\n",
    "\n",
    "pandas_183()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `extract_date_parts` that takes a Series `dates` of datetime objects and returns a DataFrame with columns 'Year', 'Month', and 'Day'.\n",
    "@check_pandas_184\n",
    "def pandas_184():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def extract_date_parts(dates):\n",
    "            return dates.dt.to_frame().assign(Year=dates.dt.year, Month=dates.dt.month, Day=dates.dt.day).drop(columns=['datetime'])\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"extract_date_parts\": extract_date_parts}\n",
    "\n",
    "pandas_184()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define a function `concat_strings_in_column` that, given a DataFrame `df` and a column `text_col`, concatenates all strings in that column with a space in between, returning the result string.\n",
    "@check_pandas_185\n",
    "def pandas_185():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def concat_strings_in_column(df, text_col):\n",
    "        return ' '.join(df[text_col].dropna().astype(str))\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"concat_strings_in_column\": concat_strings_in_column}\n",
    "\n",
    "pandas_185()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Implement a function `sort_by_index_descending` that accepts a DataFrame `df` and returns it sorted by its index in descending order.\n",
    "@check_pandas_186\n",
    "def pandas_186():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def sort_by_index_descending(df):\n",
    "        return df.sort_index(ascending=False)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sort_by_index_descending\": sort_by_index_descending}\n",
    "\n",
    "pandas_186()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `filter_by_threshold` to take a DataFrame `df` and a column name `col`, returning a DataFrame including only rows where `col` is above a given threshold.\n",
    "@check_pandas_187\n",
    "def pandas_187():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def filter_by_threshold(df, col, threshold):\n",
    "        return df[df[col] > threshold]\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"filter_by_threshold\": filter_by_threshold}\n",
    "\n",
    "pandas_187()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `get_unique_values` that takes a DataFrame `df` and a column `col`, returning a sorted array of unique values in the specified column.\n",
    "@check_pandas_188\n",
    "def pandas_188():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def get_unique_values(df, col):\n",
    "        return np.sort(df[col].dropna().unique())\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"get_unique_values\": get_unique_values}\n",
    "\n",
    "pandas_188()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Develop a function `append_row_to_dataframe` that takes a DataFrame `df` and a dictionary `row_dict`, appending the dictionary as a new row, and returning the updated DataFrame.\n",
    "@check_pandas_189\n",
    "def pandas_189():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def append_row_to_dataframe(df, row_dict):\n",
    "            return pd.concat([df, pd.DataFrame([row_dict])], ignore_index=True)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"append_row_to_dataframe\": append_row_to_dataframe}\n",
    "\n",
    "pandas_189()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Construct a function `reset_index_and_name` that takes a DataFrame `df` and returns it with its index reset and the name of the index set to 'NewIndex'.\n",
    "@check_pandas_190\n",
    "def pandas_190():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def reset_index_and_name(df):\n",
    "        df_reset = df.reset_index(drop=False)  # Reset the index and keep it as a column\n",
    "        df_reset.index.name = 'NewIndex'  # Rename the index to 'NewIndex'\n",
    "        return df_reset\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"reset_index_and_name\": reset_index_and_name}\n",
    "\n",
    "pandas_190()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `swap_dataframe_columns` that accepts a DataFrame `df` and two column names `col1` and `col2`, swapping the values of these columns.\n",
    "@check_pandas_191\n",
    "def pandas_191():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def swap_dataframe_columns(df, col1, col2):\n",
    "        df[[col1, col2]] = df[[col2, col1]]\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"swap_dataframe_columns\": swap_dataframe_columns}\n",
    "\n",
    "pandas_191()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `rename_index_label` which receives a DataFrame `df`, renames its first index label to 'FirstRow', and returns the DataFrame.\n",
    "@check_pandas_192\n",
    "def pandas_192():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def rename_index_label(df):\n",
    "        df.index = ['FirstRow' if i == 0 else x for i, x in enumerate(df.index)]\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"rename_index_label\": rename_index_label}\n",
    "\n",
    "pandas_192()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define a function `calculate_frequency_table` that, given a DataFrame `df` and a column `cat_col`, returns a DataFrame with a frequency count of each category with columns Category and count.\n",
    "@check_pandas_193\n",
    "def pandas_193():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_frequency_table(df, cat_col):\n",
    "        freq_table = df[cat_col].value_counts().reset_index()\n",
    "        freq_table.columns = ['Category', 'count']\n",
    "        return freq_table\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_frequency_table\": calculate_frequency_table}\n",
    "\n",
    "pandas_193()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Implement a function `remove_negative_entries` that takes a Series `numeric_series` and returns a Series with all negative entries removed.\n",
    "@check_pandas_194\n",
    "def pandas_194():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def remove_negative_entries(numeric_series):\n",
    "        return numeric_series[numeric_series >= 0]\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"remove_negative_entries\": remove_negative_entries}\n",
    "\n",
    "pandas_194()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `sort_column_values` that takes a DataFrame `df` and column name `col`, returning `df` sorted by `col` values in ascending order.\n",
    "@check_pandas_195\n",
    "def pandas_195():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def sort_column_values(df, col):\n",
    "        return df.sort_values(by=col, ascending=True)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sort_column_values\": sort_column_values}\n",
    "\n",
    "pandas_195()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Design a function `drop_columns_by_name` that accepts a DataFrame `df` and a list of column names `drop_cols`, removing these columns and returning the modified DataFrame.\n",
    "@check_pandas_196\n",
    "def pandas_196():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def drop_columns_by_name(df, drop_cols):\n",
    "        return df.drop(columns=drop_cols)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"drop_columns_by_name\": drop_columns_by_name}\n",
    "\n",
    "pandas_196()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Develop a function `strip_whitespace` that takes a DataFrame `df` and trims leading and trailing whitespace from all string entries.\n",
    "@check_pandas_197\n",
    "def pandas_197():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def strip_whitespace(df):\n",
    "            return df.apply(lambda col: col.str.strip() if col.dtype == 'object' else col)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"strip_whitespace\": strip_whitespace}\n",
    "\n",
    "pandas_197()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a function `duplicate_last_column` that receives a DataFrame `df` and duplicates its last column, appending the duplicate to the right of the DataFrame named as the original column with '_copy' appended.\n",
    "@check_pandas_198\n",
    "def pandas_198():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def duplicate_last_column(df):\n",
    "        last_col = df.columns[-1]  # Get the name of the last column\n",
    "        df[last_col + '_copy'] = df[last_col]  # Duplicate the last column and append it with '_copy'\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"duplicate_last_column\": duplicate_last_column}\n",
    "\n",
    "pandas_198()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write a function `filter_top_n_rows_by_column` that takes a DataFrame `df`, a column `col`, and an integer `n`, returning the top n rows sorted by values in `col`.\n",
    "@check_pandas_199\n",
    "def pandas_199():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def filter_top_n_rows_by_column(df, col, n):\n",
    "        return df.sort_values(by=col, ascending=False).head(n)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"filter_top_n_rows_by_column\": filter_top_n_rows_by_column}\n",
    "\n",
    "pandas_199()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define a function `calculate_range_of_numeric_series` that takes a Series `numeric_series` and returns the range (max - min) of its values.\n",
    "@check_pandas_200\n",
    "def pandas_200():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_range_of_numeric_series(numeric_series):\n",
    "        return numeric_series.max() - numeric_series.min()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_range_of_numeric_series\": calculate_range_of_numeric_series}\n",
    "\n",
    "pandas_200()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
